{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GFlowNet As a BSR Sampler: Demo & Tutorial\n",
    "\n",
    "In this brief tutorial, we will demonstrate the power of GFlowNet (short for GFN) -- a new generative neural network structure proposed by [Bengio et el.](https://yoshuabengio.org/2022/03/05/generative-flow-networks/) -- in Bayesian symbolic regression problems. While it's recommended to have some prior knowledge on GFN, especially on its specification (the concepts of \"flows\" and \"transitions\"), through the original paper or the official tutorial,  we will give a clear walk through over the setup of the model from scratch.\n",
    "\n",
    "The problem that the GFN in this tutorial seeks to solve is a simplified version of Symbolic Regression (SR), in which we are given a known (or confident) expression tree structure and all we search over is the specific features/operators that go into each tree node. In comparison with previous methods like BMS, this approach reduces the needs of finding structure (S), and we make such simplification since (1) we might be able to use other routines to figure out **S** (such as enumerating tree structures) and (2) this is a MVP model and we hope to inspire more complicated upgrades in the near future.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Expression Tree Representation\n",
    "\n",
    "The problem begins with a given expression tree structure **S**. A tree structure is an uninitialized binary tree (i.e. simply tell us which node is leaf, which is binary without specifying the operator/feature on those nodes). For example, we might have a structure:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    -\n",
      "   / \\\n",
      "  -   -\n",
      " /\n",
      "-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from binarytree import Node\n",
    "\n",
    "demo_tree = Node(\"-\", Node(\"-\", Node(\"-\")), Node(\"-\"))\n",
    "print(demo_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Expressions like $sin(x) + y$ or $\\frac{log(y)}{x}$ all fall under this structure, as expressed by"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     _+\n",
      "    /  \\\n",
      "  sin   y\n",
      " /\n",
      "x\n",
      " \n",
      "     _/\n",
      "    /  \\\n",
      "  log   x\n",
      " /\n",
      "y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_tree2 = Node(\"+\", Node(\"sin\", Node(\"x\")), Node(\"y\"))\n",
    "demo_tree3 = Node(\"/\", Node(\"log\", Node(\"y\")), Node(\"x\"))\n",
    "print(demo_tree2, demo_tree3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in order to better work with neural networks, we consider an alternative vector-based representation of a tree structure. Consider a tree structure **S** of depth **D**, its vector representation lives in space $\\mathbb{R}^n$ where n is the number of nodes in the full binary tree of depth **D**. We construct such vector by doing a level-by-level check on **S**, and appending a placeholder (such as 1) if there's a node and 0 otherwise. For example, the above tree structure translates to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 1, 1, 1, 0, 0, 0]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 1, 1, 1, 0, 0, 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since it only has the leftmost leaf node in the third level (the dimension of its vector is $7 = 2^3 - 1$, which is the number of nodes in a full binary tree of depth 3).\n",
    "\n",
    "On the other hand, every valid expression tree of structure **S**, denoted as **T**, is a fully instantiated tree. A fully instantiated tree can also be encoded in a vector format, but the values of that vector need to depend on the **operator space**. Consider this simple setup:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "NUM_FEATURES = 2  # e.g. the dataset has two covariates x0 and x1\n",
    "BIN_OPS = [\n",
    "    torch.add,\n",
    "    torch.mul,\n",
    "    torch.div,\n",
    "    torch.sub\n",
    "]  # 4 binary operators\n",
    "\n",
    "UN_OPS = [\n",
    "    torch.sin,\n",
    "    torch.cos,\n",
    "    torch.exp,\n",
    "    torch.abs\n",
    "]  # 4 unary operators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the vector representation, we then use 0 to denote an empty node (e.g. the right child of an unary node), 1-2 to denote the features (for leaf node), 3-6 for the 4 available binary operators, and 7-11 for the 5 unary operators. The expression $sin(x) + y$ is thus $[3, 7, 2, 1, 0, 0, 0]$ if our features are $[x, y]$. We actually have a helper function to do such conversion."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      _+\n",
      "     /  \\\n",
      "  _sin   X1\n",
      " /\n",
      "X0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tree_utils import encoding_to_tree\n",
    "ecd = torch.Tensor([3, 7, 2, 1, 0, 0, 0])\n",
    "print(encoding_to_tree(ecd))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Action and State Space\n",
    "\n",
    "Once we've decided the above representations of the tree structures, we can formally define the GFN problem that we hope to solve. Between an uninstantiated (empty) structure and a valid expression tree, we can have many intermediate **states**. An example state looks like"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   _+\n",
      "  /  \\\n",
      "sin   X1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ecd2 = torch.Tensor([3, 7, 2, 0, 0, 0, 0])\n",
    "print(encoding_to_tree(ecd2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "which is an invalid expression tree due to the lack of operand under `sin`, but once we define the **actions** on the **states**, we have the capacity of turning it into the aforementioned expression.\n",
    "\n",
    "In this simple problem setup, for a given state **X**, we simply compare its vector representation with that of the given tree structure **S**; the comparison should give us the next uninstantiated node and its type (leaf, unary, or binary). Based on these information, our action space is then the corresponding features or operators to be used for instantiating the new node. For the above example, since the leftmost leaf node is the next (and final) uninstantiated node, our action space is simply $A = \\{X_0, X_1\\}$. This formulation of the action space looks simple but restrictive -- due to the way we construct the vector representation, this method essentially builds up a valid expression tree from left to right, level by level, from an empty tree.\n",
    "\n",
    "A forward policy $\\pi$ in GFN is a stochastic mapping from the state space to the action space. This stochastic policy is usually represented as a neural network. In our case, we use a 4-layer fully connected NN (with 32 intermediate nodes, see `FTForwardPolicy` class) with `LeakyRELU` activations in between and a `sigmoid` output layer. The stochastic policy is achieved through outputting the probabilities for different actions, and we choose the action by sampling from a categorical distribution with these probabilities. Despite the stochastic policy, actions in GFN are determinstic, which means that we are guaranteed to reach a next state $s'$ with a fixed action $a'$.\n",
    "\n",
    "GFN also requires specifying a `backward policy` network, which estimates the sources of incoming flows for a given state (the `forward policy` can be thought as estimating the destinations of outgoing flows). In our setup, it's obvious that there's only one source state for a given state (you can pause and think about why; maybe revisit the action specification above?), the backward policy is trivial.\n",
    "\n",
    "<u>>Side note for anyone interested in the code</u>\n",
    "\n",
    "Since our policy is represented by a NN, the output features (# of actions) is usually fixed; but how do we account for the different available actions for different states (e.g. if the next node to instantiate is a leaf node, then we should only allow features, i.e. the action 1 & 2)? The solution is to add a `mask` function that reduces the probabilities of all \"unavailable\" actions of a given state to zero. We can thus rescale the other probabilities to make sure that the action we sample is 100% valid."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Rewards and Training\n",
    "\n",
    "In the context of symbolic regression, we grant rewards to a valid expression tree based on how this expression \"fits in\" the data we have. There are many ways of doing so, and the naive one we use is\n",
    "$$\n",
    "max(1, 100 * (1 - MSE(\\hat y, y)/TSS(y)))\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Where $TSS(y) =  ||y - \\bar{y}||^2$ is the total sum of squares for the original data (same as the one in linear regression problems) that gives the performance of a baseline predictor (i.e. use the data mean). This reward function transforms the ratio between our predictor's MSE and the baseline onto a scale between 1 and 100, with smaller MSE giving higher rewards.\n",
    "\n",
    "Once we have the rewards, we can formulate the **trajactory balance loss** (TBL) in the original paper. The details for this loss is omitted here, but the gist is that we will minimize this loss by optimizing our forward policy NN (through backpropagation with `Adam` optimizer) so that the incoming flows and outgoing flows of the states match.\n",
    "\n",
    "There are two ways to train the GFN: on-policy and off-policy trainings -- borrowing terminologies from Reinforcement Learning (RL). The difference is that on-policy training keeps using the trajectories (paths going from a scratch tree to a valid expression tree that receives rewards) generated from the under-training GFN itself; while in off-policy training we can use some other routines to generate these paths independent of the GFN under updates. We only implement on-policy training for simplicity purpose.\n",
    "\n",
    "### Step 4: Training and results\n",
    "\n",
    "The following function trains our GFN for a groundtruth expression $y = (x_1 + x_0) * x_0$ under given `batch_size` and `num_epochs`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from fixed_tree import FixedTree, FTForwardPolicy, FTBackwardPolicy\n",
    "from gflownet.gflownet import GFlowNet\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from gflownet.utils import trajectory_balance_loss\n",
    "\n",
    "def train_fixed_tree(batch_size, num_epochs):\n",
    "    X = torch.vstack([torch.empty(20).uniform_(-1, 1) for _ in range(3)]).T\n",
    "    y = (torch.sin(X[:, 1]) + X[:, 0]) * (torch.cos(X[:, 1]) + X[:, 2])\n",
    "    temp = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0])\n",
    "    env = FixedTree(temp, X, y)\n",
    "    forward_policy = FTForwardPolicy(env.state_dim, 32, env.num_actions)\n",
    "    backward_policy = FTBackwardPolicy(env.state_dim, num_actions=env.num_actions)\n",
    "    model = GFlowNet(forward_policy, backward_policy, env)\n",
    "    opt = Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "    for i in (p := tqdm(range(num_epochs))):\n",
    "        s0 = torch.zeros(batch_size, env.state_dim)\n",
    "        s, log = model.sample_states(s0, return_log=True)\n",
    "        log.back_probs.fill_(1.0)\n",
    "        loss = trajectory_balance_loss(log.total_flow,\n",
    "                                       log.rewards,\n",
    "                                       log.fwd_probs,\n",
    "                                       log.back_probs)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        if i % 10 == 0:\n",
    "            p.set_description(f\"{loss.item():.3f}\")\n",
    "\n",
    "    # s0 = one_hot(torch.zeros(10).long(), env.state_dim).float()\n",
    "    # s = model.sample_states(s0, return_log=False)\n",
    "    return model, env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We simply train with `batch_size = 32` and `num_epochs = 20000`. After the training we take 20 samples for testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6.585:  73%|███████▎  | 14563/20000 [00:53<00:19, 275.65it/s]"
     ]
    }
   ],
   "source": [
    "model, env = train_fixed_tree(64, 20000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "s0 = torch.zeros(20, env.state_dim)\n",
    "s = model.sample_states(s0, return_log = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training should be fairly fast (around 1 minute) and the sampling should be immediate (this is why GFN is also known as an amortized MCMC -- it uses longer training time to exchange ultra fast sampling process). You should also see the loss reduced hugely."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 3., 1., 1., 2., 0., 0.],\n        [4., 3., 1., 1., 2., 0., 0.],\n        [3., 4., 2., 1., 1., 0., 0.],\n        [3., 4., 1., 1., 1., 0., 0.],\n        [3., 6., 1., 1., 1., 0., 0.],\n        [3., 5., 1., 1., 1., 0., 0.],\n        [3., 5., 1., 2., 2., 0., 0.],\n        [5., 5., 2., 2., 2., 0., 0.],\n        [3., 3., 1., 2., 1., 0., 0.],\n        [4., 3., 1., 1., 2., 0., 0.],\n        [3., 6., 1., 2., 2., 0., 0.],\n        [4., 3., 1., 1., 2., 0., 0.],\n        [3., 6., 1., 2., 2., 0., 0.],\n        [3., 4., 2., 1., 1., 0., 0.],\n        [4., 4., 1., 1., 1., 0., 0.],\n        [3., 6., 1., 2., 2., 0., 0.],\n        [4., 5., 1., 1., 1., 0., 0.],\n        [5., 6., 2., 2., 2., 0., 0.],\n        [6., 4., 1., 1., 2., 0., 0.],\n        [4., 3., 1., 1., 2., 0., 0.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Out of the 20 samples, many of them are [4, 3, 1, 2, 1, 0, 0], which is the exact groud-truth solution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     ___*\n",
      "    /    \\\n",
      "  _+      X0\n",
      " /  \\\n",
      "X1   X0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(encoding_to_tree(torch.Tensor([4, 3, 1, 2, 1, 0, 0])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With around 50000 epoch of training (3-5 minutes), the samples are mostly the correct answer. To sum up, this simple demo already shows the potential of GFN in fulfilling SR tasks.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nan:   0%|          | 1/20000 [00:00<04:36, 72.39it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (32, 12)) of distribution Categorical(probs: torch.Size([32, 12])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model2, env2 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_fixed_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[18], line 19\u001B[0m, in \u001B[0;36mtrain_fixed_tree\u001B[0;34m(batch_size, num_epochs)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m (p \u001B[38;5;241m:=\u001B[39m tqdm(\u001B[38;5;28mrange\u001B[39m(num_epochs))):\n\u001B[1;32m     18\u001B[0m     s0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(batch_size, env\u001B[38;5;241m.\u001B[39mstate_dim)\n\u001B[0;32m---> 19\u001B[0m     s, log \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_states\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     log\u001B[38;5;241m.\u001B[39mback_probs\u001B[38;5;241m.\u001B[39mfill_(\u001B[38;5;241m1.0\u001B[39m)\n\u001B[1;32m     21\u001B[0m     loss \u001B[38;5;241m=\u001B[39m trajectory_balance_loss(log\u001B[38;5;241m.\u001B[39mtotal_flow,\n\u001B[1;32m     22\u001B[0m                                    log\u001B[38;5;241m.\u001B[39mrewards,\n\u001B[1;32m     23\u001B[0m                                    log\u001B[38;5;241m.\u001B[39mfwd_probs,\n\u001B[1;32m     24\u001B[0m                                    log\u001B[38;5;241m.\u001B[39mback_probs)\n",
      "File \u001B[0;32m~/Documents/GitHub/gflownet/gflownet/gflownet.py:71\u001B[0m, in \u001B[0;36mGFlowNet.sample_states\u001B[0;34m(self, s0, return_log)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m     70\u001B[0m     probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_probs(s[\u001B[38;5;241m~\u001B[39mdone])\n\u001B[0;32m---> 71\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[43mCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m     72\u001B[0m     s[\u001B[38;5;241m~\u001B[39mdone] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mupdate(s[\u001B[38;5;241m~\u001B[39mdone], actions)\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_log:\n",
      "File \u001B[0;32m~/Documents/GitHub/gflownet/venv/lib/python3.9/site-packages/torch/distributions/categorical.py:64\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[0;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     63\u001B[0m batch_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39mndimension() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mSize()\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mCategorical\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbatch_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/gflownet/venv/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[0;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[1;32m     53\u001B[0m         valid \u001B[38;5;241m=\u001B[39m constraint\u001B[38;5;241m.\u001B[39mcheck(value)\n\u001B[1;32m     54\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m---> 55\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     56\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     58\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof distribution \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto satisfy the constraint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(constraint)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     60\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     61\u001B[0m             )\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28msuper\u001B[39m(Distribution, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mValueError\u001B[0m: Expected parameter probs (Tensor of shape (32, 12)) of distribution Categorical(probs: torch.Size([32, 12])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "model2, env2 = train_fixed_tree(32, 20000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Potential Next Steps\n",
    "\n",
    "There are many ways that this MVP GFN model can be improved. To name a few priority items:\n",
    "\n",
    "- encoder: we propose a vector-based encoder schema for an expression tree object. Under this setup the vector will contain lots of sparsity (especially when the tree grows deeper). Other alternative encoders include Tree-RNN, VAE, or a stripped version of the current vector representation.\n",
    "- action: our current actions assign features/operators according to a given template (structure) from left to right, level by level. There might be other ways to formulate the action space so it fits in a natural construction process better.\n",
    "- reward function: there can be other reward functions (e.g. likelihood-based, utility theory related, etc.) other than the current one based on MSE.\n",
    "- structure proposal: simultaneously training a structure proposal GFN at the same time?\n",
    "- ...: let's come together and think about more."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
